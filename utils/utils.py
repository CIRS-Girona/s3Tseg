# Modified by Hayat Rajani (hayatrajani@gmail.com)
#
# Copyright (c) Facebook, Inc. and its affiliates.
# 
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# 
#     http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Misc functions.
Mostly copy-paste from torchvision references or other public repos like DETR and timm:
https://github.com/facebookresearch/detr/blob/master/util/misc.py
https://github.com/rwightman/pytorch-image-models
"""


import os
import sys
import math
import numpy
import torch
import torch.nn as nn
import torch.distributed as D
import warnings
import collections.abc
from itertools import repeat


def to_2tuple(x):
    if isinstance(x, collections.abc.Iterable):
        return x
    return tuple(repeat(x, 2))


def fix_random_seeds(seed=42):
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    numpy.random.seed(seed)


def is_dist_avail_and_initialized():
    if not D.is_available():
        return False
    if not D.is_initialized():
        return False
    return True


def get_world_size():
    if not is_dist_avail_and_initialized():
        return 1
    return D.get_world_size()


def get_rank():
    if not is_dist_avail_and_initialized():
        return 0
    return D.get_rank()


def is_main_process():
    return get_rank() == 0


def save_on_master(*args, **kwargs):
    if is_main_process():
        torch.save(*args, **kwargs)


def setup_for_distributed(is_master):
    """
    This function disables printing when not in master process
    """
    import builtins as __builtin__
    builtin_print = __builtin__.print

    def print(*args, **kwargs):
        force = kwargs.pop('force', False)
        if is_master or force:
            builtin_print(*args, **kwargs)

    __builtin__.print = print


def init_distributed_mode(args):
    # launched with torchrun or torch.distributed.run
    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:
        args.rank = int(os.environ["RANK"])
        args.gpu = int(os.environ['LOCAL_RANK'])
    else:
        print('Code is not suited for non-distributed training. Exitting...')
        sys.exit(1)

    D.init_process_group(
        backend="nccl",
        init_method=args.distr_url
    )
    torch.cuda.set_device(args.gpu)

    print('| distributed init (rank {}): {}'.format(
        args.rank, args.distr_url), flush=True)
    
    D.barrier()
    setup_for_distributed(args.rank == 0)


def has_batchnorms(model):
    bn_types = (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d, nn.SyncBatchNorm)
    for _, module in model.named_modules():
        if isinstance(module, bn_types):
            return True
    return False


def get_params_groups(model):
    regularized = []
    not_regularized = []
    for name, param in model.named_parameters():
        if not param.requires_grad:
            continue
        # we do not regularize biases nor Norm parameters
        if name.endswith(".bias") or len(param.shape) == 1:
            not_regularized.append(param)
        else:
            regularized.append(param)
    return [{'params': regularized}, {'params': not_regularized, 'weight_decay': 0.}]


def clip_gradients(model, clip):
    norms = []
    for name, p in model.named_parameters():
        if p.grad is not None:
            param_norm = p.grad.data.norm(2)
            norms.append(param_norm.item())
            clip_coef = clip / (param_norm + 1e-6)
            if clip_coef < 1:
                p.grad.data.mul_(clip_coef)
    return numpy.mean(norms)


def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # type: (Tensor, float, float, float, float) -> Tensor
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def polyLR_scheduler(base_value, epochs, niter_per_ep, warmup_epochs=0):
    max_iters = epochs * niter_per_ep
    warmup_iters = warmup_epochs * niter_per_ep
    iters = numpy.arange(max_iters - warmup_iters)
    
    warmup_schedule = numpy.array([])
    if warmup_epochs > 0:
        warmup_schedule = numpy.linspace(0, base_value, warmup_iters)
    schedule = base_value * (1.0 - iters / max_iters) ** 0.9
    schedule = numpy.concatenate((warmup_schedule, schedule))
    
    assert len(schedule) == max_iters
    return schedule


def stepLR_scheduler(base_value, epochs, niter_per_ep, gamma=0.1, step=10):
    max_iters = epochs * niter_per_ep
    epochs = numpy.arange(epochs//step)
    schedule = numpy.repeat(base_value * gamma ** epochs, step*niter_per_ep)
    #schedule = numpy.pad(schedule, (0,max_iters-len(schedule)), 'edge')
    schedule = numpy.concatenate((
        schedule, numpy.repeat(schedule[-1], max_iters-len(schedule))
    ))
    assert len(schedule) == max_iters
    return schedule


def constLR_scheduler(base_value, epochs, niter_per_ep):
    max_iters = epochs * niter_per_ep
    schedule = numpy.repeat(base_value, max_iters)
    assert len(schedule) == max_iters
    return schedule


def resume_from_checkpoint(ckp_path, run_variables=None, **kwargs):
    if not os.path.isfile(ckp_path):
        return
    print("Found checkpoint at {}".format(ckp_path))
    checkpoint = torch.load(ckp_path, map_location="cpu")

    # key is what to look for in the checkpoint file
    # value is the object to load
    # example: {'state_dict': model}
    for key, value in kwargs.items():
        if key in checkpoint and value is not None:
            try:
                msg = value.load_state_dict(checkpoint[key], strict=False)
                print("=> loaded '{}' from checkpoint '{}' with msg {}".format(key, ckp_path, msg))
            except TypeError:
                try:
                    msg = value.load_state_dict(checkpoint[key])
                    print("=> loaded '{}' from checkpoint: '{}'".format(key, ckp_path))
                except ValueError:
                    print("=> failed to load '{}' from checkpoint: '{}'".format(key, ckp_path))
        else:
            print("=> key '{}' not found in checkpoint: '{}'".format(key, ckp_path))

    # re load variable important for the run
    if run_variables is not None:
        for var_name in run_variables:
            if var_name in checkpoint:
                run_variables[var_name] = checkpoint[var_name]


def load_pretrained_weights(model, pretrained_weights, mode='train'):
    assert mode.lower() in ('finetune','train','eval'), \
        "Invalid mode argument. Must be 'finetune', 'train' or 'eval'"
    if not os.path.isfile(pretrained_weights):
        return
    print('Found pretrained weights at {}'.format(pretrained_weights))
    state_dict = torch.load(pretrained_weights, map_location="cpu")
    if mode.lower() == 'eval':
        msg = model.load_state_dict(state_dict, strict=False)
    elif mode.lower() == 'train':
        msg = model.module.encoder.load_state_dict(state_dict, strict=False)
    else:
        # finetune previously trained model on additional/fewer classes
        # number of classes can be set in the config file
        # weights/bias initialized with defualt scheme while building the model
        default_state_dict = model.module.state_dict()
        assert default_state_dict['decoder.head.weight'].shape[1] \
            == state_dict['decoder.head.weight'].shape[1], \
            "Incompatible input dimensions for classifier! Found %d expected %d" %(
                state_dict['decoder.head.weight'].shape[1],
                default_state_dict['decoder.head.weight'].shape[1]
            )
        # collect keys corresponding to output layer weights (and bias, if any)
        keys = [k for k in state_dict.keys() if 'decoder.head' in k]
        for k in keys:
            # explicitly replace with default init values
            state_dict[k] = default_state_dict[k]
        msg = model.module.load_state_dict(state_dict, strict=False)
    print('Pretrained weights loaded with msg: {}'.format(msg))
